---
title: "PML Course Project"
author: "Xavi Martí Bofill"
date: "4/4/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Practical Machine Learning - Final Course Project

This is the final course project for 'Practical Machine Learning', 8th subject from 'Data Science Specialization', MOOC in Coursera imparted by Jeff Leek, Brian Caffo and Roger D Peng, all of them John Hopkins University professors. You can find more information @ https://www.coursera.org/learn/practical-machine-learning/home/welcome.

This work is done by me, Xavier Martí Bofill.

### Introduction

Exercise introduction: “Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ??? a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).”

We will use this data to predict the kind of movement the subject is performing using Machine Learning Algorithms.

### Dataset cleaning

The training and test data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

For this code to run properly we must have both training and test files already
in working directory.

```{r loads, cache=TRUE}
library(caret); library(rpart.plot); library(randomForest); library(rpart)
training <- read.csv('pml-training.csv', stringsAsFactors = T)
testing <- read.csv('pml-testing.csv', stringsAsFactors = T)
lapply(list(training, testing), dim)
```
Once loaded, we'll proceed to eliminate all irrelevant features. NAs, void values, and finally ID, date and time.

```{r NAs, cache=TRUE}
nas <- apply(training, 2, function(x) sum(is.na(x)))
table(nas)
na_feats <- names(nas)[nas == 19216]
training <- training[,!(names(training) %in% na_feats)]
```

```{r empties, cache=TRUE}
empties <- apply(training, 2, function(x) sum(x == ""))
table(empties)
empty_feats <- names(empties)[empties == 19216]
training <- training[,!(names(training) %in% empty_feats)]
```

#### Dates and Times
You can check that activity type in this dataset is very dependant on date / time combination, but it would be erroneous to assume this generalizes to new data. Using this data, knowing that test data comes from the same dataset, would be totally cheating in real life (and useless). We will remove it along with ID column, not to interfere with our ML algs.
Column 1 is ID, columns 3 to 5 are date/time related.
Finally, we will apply all changes to test set.

```{r d&t, cache=TRUE}
training <- training[,-c(1, 3:5)]
testing <- testing[, which(names(testing) %in% names(training))]
```

### Data Partition

Dividing data to test our models in a validation set.

```{r CV, cache=TRUE}
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
Train <- training[inTrain, ]; Test <- training[-inTrain, ]
dim(Train); dim(Test)
```

### Classification Tree

We will use rpart function to develop a single classification. We will plot it using rpart.plot package and finally check in the Confusion Matrix its accuracy.

```{r CT, cache=TRUE}
ct_mod <- rpart(classe ~ ., data=Train, method="class")
prp(ct_mod)
confusionMatrix(predict(ct_mod, Test, type = "class"), Test$classe)
```

Accuracy is still far from ideal. Let's check whether combining multiple trees can help improve it.

### Random Forest

```{r RF, cache=TRUE}
rf_mod <- randomForest(classe ~ ., data=Train)
confusionMatrix(predict(rf_mod, Test, type = 'class'), Test$classe)
```

That's what I call an improvement!

### Solution Generation

I have found out that some classes between training and testing do not match. We will need to coerce that previously to writing the solution.
```{r class, cache=TRUE}
missclass <- which(!(sapply(testing, class) == sapply(training[, 1:55], class)))
for(i in missclass){testing[, i] <- as.numeric(testing[, i])}
```

```{r sol, cache = TRUE, eval=FALSE}
solution <- predict(rf_mod, testing, type = 'class')
write.csv(data.frame(1:20, testing$classe), file = "solution.csv")
```